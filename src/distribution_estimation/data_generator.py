import numpy as np
import tensorflow as tf
import math

import pdf_functions


class GaussianMixture(object):
    """Class for generating data from known Gaussian mixtures, for experimenting with distribution estimation.

    Instantiations of this class are specific to a single randomly generated Gaussian mixture in d dimensions, where
    each Gaussian has the same covariance matrix Sigma. Let Sigma = At A where A is also positive definite. The matrix
    A is generated by a random matrix of orthogonal eigen_vectors Q and a uniform random choice of eigenvalues L,
    in between the bounds specified by min_eigenvalue and min_eigenvalue (specified in distribution_configuration).
    A = QLQt.
    """
    def __init__(self, conf, random):
        """ Constructs a DataGenerator in the contxt of a DensityConfiguration for Kernel Density Estimation

        Args:
            conf: A DensityConfiguration - used to specify the properties of the Gaussian
            random: Random - Abstract's random behaviour such drawing the standard normal samples.
        """
        self.random = random
        self.d = conf.d
        min_eigenvalue = conf.min_eigenvalue
        max_eigenvalue = conf.max_eigenvalue
        self.means = conf.means.astype(np.float32)
        self.number_of_means = self.means.shape[0]
        # Generate a standard deviation matrix A, by producing an eigen decomposition with a minimum limit for each
        # eigenvalue
        self.actual_A = conf.fixed_A
        if self.actual_A is None:
            Q = _random_orthogonal_matrix(self.d)
            eigenvalues = random.uniform(min_eigenvalue, max_eigenvalue, self.d)
            upper_lambda = np.eye(self.d) * eigenvalues
            self.actual_A = np.matmul(Q, np.matmul(upper_lambda, Q.transpose())) # * np.exp(-1.0)
            # # TODO(Jack) remove
            # self.sigma = np.matmul(self.actual_A.transpose(), self.actual_A)
            # self.sigma_determinant = np.linalg.det(self.sigma)
            # self.sigma_inverse = np.linalg.inv(self.sigma).astype(np.float32)
        else:
            eigenvalues, Q = np.linalg.eig(self.actual_A)
        self.Q = Q.astype(np.float32)
        self.lam_inv = 1.0 / eigenvalues.astype(np.float32)
        self.actual_A = self.actual_A.astype(np.float32)

    def actual_values(self):
        return self.actual_A, self.lam_inv, self.means


    def sample(self, n):
        """Generates an [n, d] numpy array, where the n elements are drawn from a mixture of Gaussians in d dimensions

        Args:
            conf: The density_estimation configuration
            random: Service object encapsulating random behavior
            actual_A: The standard deviation matrix, the same one is applied across each Gaussian

        Return: A [n, d] numpy array of floating point numbers, drawn from the Gaussian mixture.
        """
        chosen_means = self.random.choice(self.means, n, replace=True)

        z = self.random.normal_numpy_array([n, self.d])
        x = chosen_means + np.matmul(z, self.actual_A)
        return x.astype(np.float32)

    def pdf(self, a, batch_size):
        """ Compute the PDF of the Gaussian mixture associated with this data generator, for batch_size points in a.

        Args:
            a: A [batch_size, d] tensor of observations in the distribution space (doesn't neccessarily have to be
            drawn from the Gaussian mixture).
            batch_size: A tensor, the number of examples in a.

        Return:
            p(a) - The value of the pdf at for each point in a.
        """
        px, _ = pdf_functions.gaussian_mixture(a, self.means, self.actual_A, batch_size, self.d)
        #TODO(Jack) remove
        # distance_squared = self._distance_squared(a, batch_size)
        # exponent = 0.5 * (-distance_squared)
        # pa_unnormed = tf.reduce_sum(tf.exp(exponent), axis=1)
        # # pa = 1.0 / (self.sigma_determinant ** 0.5 * self.number_of_means) * pa_unnormed # (2.0 * math.pi) ** self.d *
        # pa = 1.0 / (((2.0 * math.pi) ** self.d * self.sigma_determinant) ** 0.5 * self.number_of_means) * pa_unnormed
        # return pa, distance_squared

    def distance_distribution(self, a, batch_size):
        """
        For batch_size points in a, report the probability that said point occurs that far from the mean. (Being a
        Gaussian, this is given by a chi-squared distribution).

        Args:
            a: A [batch_size, d] tensor of points in the space of the Gaussian

        Returns:
            A [batch_size] tensor, that represents the probability each point occurs as far as it does from the mean.
        """
        distance_squared = self._distance_squared(a, batch_size)
        return tf.reduce_mean(pdf_functions.chi_squared_distribution(distance_squared), axis=1)


    def _distance_squared(self, a, batch_size):
        a = tf.reshape(a, [batch_size, 1, self.d])
        means = self.means.reshape(1, self.number_of_means, self.d)
        difference = a - means
        difference = tf.reshape(difference, [batch_size, self.number_of_means, self.d, 1])
        distance_squared = tf.reshape(tf.tensordot(difference, self.sigma_inverse, axes=[[2], [0]]),
                              [batch_size, self.number_of_means, 1, self.d])
        distance_squared = tf.matmul(distance_squared, difference)
        distance_squared = tf.reshape(distance_squared, [batch_size, self.number_of_means])
        return distance_squared

def _random_orthogonal_matrix(d):
    H = np.eye(d)
    D = np.ones((d,))
    for n in range(1, d):
        x = np.random.normal(size=(d - n + 1,))
        D[n - 1] = np.sign(x[0])
        x[0] -= D[n - 1] * np.sqrt((x * x).sum())
        # Householder transformation
        Hx = (np.eye(d - n + 1) - 2. * np.outer(x, x) / (x * x).sum())
        mat = np.eye(d)
        mat[n - 1:, n - 1:] = Hx
        H = np.dot(H, mat)
        # Fix the last sign such that the determinant is 1
    D[-1] = (-1) ** (1 - (d % 2)) * D.prod()
    # Equivalent to np.dot(np.diag(D), H) but faster, apparently
    Q = (D * H.T).T
    return Q.astype(np.float32)